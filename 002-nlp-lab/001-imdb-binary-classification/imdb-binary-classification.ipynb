{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH2doG9_IP3h"
      },
      "source": [
        "## **캡스톤치맥회동 AI 스터디 python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytg2OyeZCniN"
      },
      "source": [
        "### 스터디 개요\n",
        "1. HuggingFace Hub의 imdb 데이터셋을 이용한 전처리 및 텍스트 토큰화\n",
        "2. 학습, 검증, 테스트 로직 구현\n",
        "3. 모델 성능 향상\n",
        "\n",
        "### 스터디로 얻어갈 수 있는 능력\n",
        "* HuggingFace Hub에서 데이터셋을 불러올 수 있습니다.\n",
        "* train, validation, test 데이터셋을 분리할 수 있습니다.\n",
        "* 텍스트 데이터를 전처리하고, 임베딩할 수 있습니다.\n",
        "* 학습, 검증, 테스트 로직을 익힐 수 있습니다.\n",
        "\n",
        "> [TODO]에 코드를 채워넣으면 됩니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD4zCh3_eBO5"
      },
      "source": [
        "## 1. 데이터셋 불러오기\n",
        "* 허깅페이스의 imdb 데이터셋을 불러옵니다.\n",
        "* 참고) imdb는 train, test 데이터셋을 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q3Pv0SUkd53B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\AI\\002-nlp-lab\\001-imdb-binary-classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset  # HuggingFace에서 지원하는 라이브러리입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SQ3B-Xf4fyZz"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "test_dataset = load_dataset(\"imdb\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn-q-4VNiiUE",
        "outputId": "ba20254e-ae62-4fa0-fa6d-279e90381b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset 개수: 25000\n",
            "test dataset 개수: 25000\n",
            "\n",
            "train dataset 모양\n",
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 25000\n",
            "})\n",
            "\n",
            "test dataset 모양\n",
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 25000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 길이 확인\n",
        "print(\"train dataset 개수:\", len(train_dataset))\n",
        "print(\"test dataset 개수:\", len(test_dataset))\n",
        "\n",
        "# 데이터셋 모양 확인\n",
        "print(\"\\ntrain dataset 모양\")\n",
        "print(train_dataset)\n",
        "\n",
        "print(\"\\ntest dataset 모양\")\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SD_-t2b-kBcF",
        "outputId": "cc6c087a-eee1-4ade-90fc-d5e1bb05c7ad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If only to avoid making this type of film in t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This film was probably inspired by Godard's Ma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
              "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
              "2  If only to avoid making this type of film in t...      0\n",
              "3  This film was probably inspired by Godard's Ma...      0\n",
              "4  Oh, brother...after hearing about this ridicul...      0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dataset 객체를 전처리 가능한 구조로 변환합니다.\n",
        "train_df = pd.DataFrame(train_dataset)\n",
        "test_df = pd.DataFrame(test_dataset)\n",
        "\n",
        "# 2번에서 train : validation : test 데이터셋을 80 : 10 : 10으로 분할하기 위해, 전체 데이터셋으로 합칩니다.\n",
        "df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
        "\n",
        "# 5개의 텍스트를 출력합니다.\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqvS44dJeFxn"
      },
      "source": [
        "## 2. Train/Val/Test 데이터셋으로 분리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CyMrC_5_khGa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PytyI2iCkVfI"
      },
      "outputs": [],
      "source": [
        "X = df.text  # df[\"text\"]로 써도 됩니다.\n",
        "y = df.label\n",
        "\n",
        "# 1차 분리: train : tmp = 80 : 20\n",
        "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y  # label의 분포를 유지하기 위해 stratify를 명시해줍니다.\n",
        ")\n",
        "\n",
        "# 2차 분리: train : val : test = 80 : 10 : 10\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.5, stratify=y_tmp\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyFaLdKleDxV"
      },
      "source": [
        "## 3. 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZWqUUSooif4v"
      },
      "outputs": [],
      "source": [
        "import re  # regex: 정규 표현식을 지원하는 라이브러리입니다.\n",
        "\n",
        "# [TODO 1]\n",
        "def clean_text(text):\n",
        "    # [TODO 1-1]. 소문자로 변환합니다.\n",
        "    text = text.lower()\n",
        "\n",
        "    # [TODO 1-2]. 특수문자, 불필요한 문자를 제거하여, 알파벳과 공백만 남깁니다.\n",
        "    text = re.sub(r\"^[a-z\\s]\", \"\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1LDLl0-xnSaS"
      },
      "outputs": [],
      "source": [
        "# 훈련, 검증, 테스트 데이터셋에 전처리 함수를 적용합니다.\n",
        "X_train_clean = X_train.apply(clean_text)\n",
        "X_val_clean = X_val.apply(clean_text)\n",
        "X_test_clean = X_test.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "p3QdfB62ouqF",
        "outputId": "bd0c3614-2b47-4a2f-c563-66e0932bd447"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>before_text</th>\n",
              "      <th>after_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Having a close experience with one such patien...</td>\n",
              "      <td>aving a close experience with one such patient...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is by far one of the worst movies i have ...</td>\n",
              "      <td>his is by far one of the worst movies i have e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film is a great disappointment. Director ...</td>\n",
              "      <td>his film is a great disappointment. director v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When I first popped in Happy Birthday to Me, I...</td>\n",
              "      <td>hen i first popped in happy birthday to me, i ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I signed in just to comment on how awfully stu...</td>\n",
              "      <td>signed in just to comment on how awfully stup...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         before_text  \\\n",
              "0  Having a close experience with one such patien...   \n",
              "1  This is by far one of the worst movies i have ...   \n",
              "2  This film is a great disappointment. Director ...   \n",
              "3  When I first popped in Happy Birthday to Me, I...   \n",
              "4  I signed in just to comment on how awfully stu...   \n",
              "\n",
              "                                          after_text  \n",
              "0  aving a close experience with one such patient...  \n",
              "1  his is by far one of the worst movies i have e...  \n",
              "2  his film is a great disappointment. director v...  \n",
              "3  hen i first popped in happy birthday to me, i ...  \n",
              "4   signed in just to comment on how awfully stup...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "before_and_after = pd.concat([X_train.head(), X_train_clean.head()], axis=1).reset_index(drop=True)\n",
        "before_and_after.columns = [\"before_text\", \"after_text\"]\n",
        "\n",
        "# 전처리 전후 텍스트를 5개 출력합니다.\n",
        "before_and_after"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJKOaK3wqQZH"
      },
      "source": [
        "## 4. 데이터 임베딩하기\n",
        "* 임베딩: 텍스트 -> 벡터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwN9LyIkMDeu"
      },
      "source": [
        "### 4-1. tokenizer 구축하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9OzYkIbrK3Eq"
      },
      "outputs": [],
      "source": [
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "z7i_95ryKY2X"
      },
      "outputs": [],
      "source": [
        "# [TODO 2]\n",
        "def tokenize(sentence: str) -> List[str]:\n",
        "    # 문장을 토큰으로 쪼갭니다.\n",
        "    tokens = re.split(r'\\s+|([.,!?]|n\\'t|\\'\\w+)', sentence)\n",
        "\n",
        "    # [TODO 2-1] 토큰에서 공백을 제거합니다.\n",
        "    tokens = [token for token in tokens if token != \" \" or token != None]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1lJSCTTMJlK"
      },
      "source": [
        "### 4-2. vocabulary 구축하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A4B4si5gqcNw"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Zz8VJMnjrKwK"
      },
      "outputs": [],
      "source": [
        "# [TODO 3]\n",
        "def build_vocab(\n",
        "    sentences: List[str],\n",
        "    min_freq=5,\n",
        "    tokenize=tokenize\n",
        ") -> Dict[str, int]:  # 키(str), 값(int)\n",
        "\n",
        "    # [TODO 3-1] # 모든 문장의 토큰을 하나의 리스트로 모읍니다.\n",
        "    # 힌트: 평탄화된 토큰 리스트를 만들기 위해, append 대신 extend를 사용합니다.\n",
        "    all_tokens = []\n",
        "    for sentence in sentences:\n",
        "        all_tokens.extend(tokenize(sentence))\n",
        "\n",
        "    # 각 토큰의 빈도를 셉니다.\n",
        "    word_counts = Counter(all_tokens)\n",
        "\n",
        "    # [TODO 3-2] Vocabulary를 구성합니다.\n",
        "    # 최소 빈도(freq)보다 더 많이 등장한 단어만 id2token에 저장합니다.\n",
        "    # id2token = [\"UNK\", \"this\", \"is\", \"an\", \"apple\", ...]\n",
        "    # [UNK] 토큰의 id는 0으로 처리합니다.\n",
        "    id2token = [\"UNK\"]\n",
        "    for word, count in word_counts.items():\n",
        "        if count >= min_freq:\n",
        "            id2token.append(word)\n",
        "\n",
        "    # [TODO 3-3] id2token을 기반으로 token2id를 구축합니다.\n",
        "    # token2id = {\"UNK\": 0, \"this\": 1, \"is\": 2, ...}\n",
        "    token2id = {}\n",
        "    for id, word in enumerate(id2token):\n",
        "        token2id[word] = id\n",
        "\n",
        "    return token2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IF-pptaQa_RB"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터셋의 텍스트로 사전을 구축합니다.\n",
        "token2id = build_vocab(X_train_clean.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyn7Yfl0ZU3i"
      },
      "source": [
        "### 4-3. 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xrwx5aKMrQws"
      },
      "outputs": [],
      "source": [
        "# [TODO 4]\n",
        "def encode(\n",
        "    sentence: str,\n",
        "    tokenize=tokenize,\n",
        "    token2id=token2id,\n",
        "    max_seq_len=128\n",
        ") -> List[int]:\n",
        "    # 주어진 tokenize 함수를 사용하여 문장의 단어 목록으로 변환합니다.\n",
        "    tokens = tokenize(sentence)\n",
        "\n",
        "    # [TODO 4-1] 토큰을 ID로 변환해, 리스트로 저장합니다.\n",
        "    token_ids = [token2id.get(token, 0) for token in tokens]\n",
        "\n",
        "    if len(token_ids) < max_seq_len:\n",
        "        # [TODO 4-2] 시퀀스의 길이를 맞추기 위해 0 패딩을 적용합니다.\n",
        "        token_ids.extend([0] * (max_seq_len - len(token_ids)))\n",
        "    else:\n",
        "        # [TODO 4-3] 시퀀스가 너무 길 때, 지정된 최대 길이에 맞게 잘라냅니다.\n",
        "        token_ids = token_ids[:max_seq_len]\n",
        "\n",
        "    return token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i0TiA3t3rWLk"
      },
      "outputs": [],
      "source": [
        "# train, val, test 데이터셋에서 setence를 모두 토큰으로 변환합니다.\n",
        "X_train_vec = [encode(sentence) for sentence in X_train_clean]\n",
        "X_val_vec = [encode(sentence) for sentence in X_val_clean]\n",
        "X_test_vec = [encode(sentence) for sentence in X_test_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNyBAbbNgCf8",
        "outputId": "d39d9ec8-f030-415c-d41e-7ef7f3ed0c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "임베딩 전: his is my favourite indian movie of all time. it is comic genius. salman khan is hilarious. but amir khan steals the show with his witty dialogue. karisma kapoor's outfits tell a story of their own - makes you wonder if the stylist deliberately made her wear some of the clothes just to make the movie funnier (at one point she looks like she's wearing a nappy). andaz apna apna is the only comedy genre movie to make me laugh from the beginning till the very end. there is not one dull moment, every scene is hilarious, even the songs and dance moves will have you in stitches of laughter. i especially loved the scene in which amar (amir khan) 'regains his memory'. i've seen this movie so many times i've lost count. and i'm so glad to say that this time bollywood can take all the credit for this fantastic movie as far as i know a.a.a it is not a replicate of a hollywood movie (thank god). overall i recommend this movie to anyone who understands hindi/ urdu and loves good comedy.<br /><br />watch it you'll love it!!!!!\n",
            "임베딩 후: [43, 2, 10, 2, 18, 2, 671, 2, 672, 2, 115, 2, 46, 2, 166, 2, 145, 24, 25, 2, 130, 2, 10, 2, 673, 2, 674, 24, 25, 2, 675, 2, 676, 2, 10, 2, 501, 24, 25, 2, 69, 2, 677, 2, 676, 2, 678, 2, 12, 2, 158, 2, 6, 2, 43, 2, 679, 2, 680, 24, 25, 2, 681, 2, 682, 149, 25, 2, 683, 2, 299, 2, 3, 2, 178, 2, 46, 2, 375, 2, 684, 2, 685, 2, 224, 2, 271, 2, 686, 2, 132, 2, 12, 2, 687, 2, 688, 2, 689, 2, 127, 2, 690, 2, 84, 2, 46, 2, 12, 2, 691, 2, 58, 2, 35, 2, 662, 2, 12, 2, 115, 2, 692, 2, 693, 2, 7, 2]\n"
          ]
        }
      ],
      "source": [
        "print(\"임베딩 전:\", X_train_clean.iloc[5])\n",
        "print(\"임베딩 후:\", X_train_vec[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmuNCL3wePGH"
      },
      "source": [
        "## 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndT60TjepxZ0",
        "outputId": "7c8e5378-0616-4b29-b831-e1a0c98bc989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습을 완료했습니다.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\AI\\002-nlp-lab\\001-imdb-binary-classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=100).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# [TODO 5] X_train_vec, y_train을 기반으로 학습을 진행합니다.\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "print(\"학습을 완료했습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypm0GOTvebl1"
      },
      "source": [
        "## 6. 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPsg3Mo9hqeV",
        "outputId": "0ba10ce0-5ba1-4a9d-e8b7-f0a900779d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검증 데이터 정확도: 0.4998\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# [TODO 6] X_val_vec, y_val을 기반으로 검증을 진행합니다.\n",
        "y_val_pred = model.predict(X_val_vec)\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"검증 데이터 정확도:\", val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_AmYEMZed5Q"
      },
      "source": [
        "## 7. 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA1wRujjeFXO",
        "outputId": "c595dbf1-7387-4552-afc4-24f400d196ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 데이터 정확도: 0.5064\n",
            "TN: 753, FP: 1747\n",
            "FN: 721, TP: 1779\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_test_pred = model.predict(X_test_vec)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"테스트 데이터 정확도:\", test_accuracy)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# 긍정(1)과 부정(0) 레이블의 의미를 해석할 수 있도록 출력합니다.\n",
        "# TN (True Negative): 실제 0을 0으로 맞힘\n",
        "# FP (False Positive): 실제 0을 1로 틀림 (오탐)\n",
        "# FN (False Negative): 실제 1을 0으로 틀림 (미탐)\n",
        "# TP (True Positive): 실제 1을 1로 맞힘\n",
        "print(f\"TN: {conf_matrix[0, 0]}, FP: {conf_matrix[0, 1]}\")\n",
        "print(f\"FN: {conf_matrix[1, 0]}, TP: {conf_matrix[1, 1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
