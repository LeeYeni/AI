{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116ba3eb",
   "metadata": {},
   "source": [
    "## **캡스톤치맥회동 AI 스터디 python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711dfdc",
   "metadata": {},
   "source": [
    "### 스터디 개요\n",
    "\n",
    "1. TAPT(Task-Adaptive-Pretraining)을 통한 성능 고도화\n",
    "\n",
    "2. Optuna를 활용한 최적의 하이퍼파라미터 탐색\n",
    "\n",
    "\n",
    "\n",
    "### 스터디로 얻어갈 수 있는 능력\n",
    "\n",
    "* HuggingFace Hub에서 제공하는 토크나이저 및 모델을 사용할 수 있습니다.\n",
    "\n",
    "* MLM(Masked Language Model) 기반의 TAPT를 적용하여, 파인튜닝 전 도메인 적합성을 높일 수 있습니다.\n",
    "\n",
    "* Optuna 프레임워크를 통해 최적의 하이퍼파라미터를 자동으로 찾을 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "> [TODO]에 코드를 채워넣으면 됩니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380a3b0",
   "metadata": {},
   "source": [
    "## **0. 난수 시드 고정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb947425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)          # Pytorch CPU 연산 난수 시드를 고정합니다.\n",
    "    torch.cuda.manual_seed_all(seed) # GPU 연산 난수 시드를 고정합니다.\n",
    "\n",
    "    # 매 실행마다 동일한 결과를 재현합니다.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adb72a",
   "metadata": {},
   "source": [
    "## **1. 데이터셋 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede3c998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AI\\002-nlp-lab\\002-imdb-tapt-optuna\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca28c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset 개수: 25000\n",
      "test dataset 개수: 25000\n",
      "\n",
      "train dataset 모양\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "\n",
      "test dataset 모양\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 길이 확인\n",
    "print(\"train dataset 개수:\", len(train_dataset))\n",
    "print(\"test dataset 개수:\", len(test_dataset))\n",
    "\n",
    "# 데이터셋 모양 확인\n",
    "print(\"\\ntrain dataset 모양\")\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"\\ntest dataset 모양\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84060d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset 객체를 전처리 가능한 구조로 변환합니다.\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "test_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "# 2번에서 train : validation : test 데이터셋을 80 : 10 : 10으로 분할하기 위해, 전체 데이터셋으로 합칩니다.\n",
    "df = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "\n",
    "# 5개의 텍스트를 출력합니다.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f3fc8",
   "metadata": {},
   "source": [
    "## **2. 데이터셋 분할**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef421a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.text\n",
    "y = df.label\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14d149",
   "metadata": {},
   "source": [
    "## **3. 데이터셋 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b1c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 텍스트를 소문자로 변환합니다.\n",
    "    text = text.lower()\n",
    "\n",
    "    # 소문자와 공백을 제외한 문자를 제거합니다.\n",
    "    text = re.sub(r\"^[a-z\\s]\", \"\", text)\n",
    "\n",
    "X_train_clean = X_train.apply(clean_text)\n",
    "X_val_clean = X_val.apply(clean_text)\n",
    "X_test_clean = X_test.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25a2fe",
   "metadata": {},
   "source": [
    "## **4. 토크나이저 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188f2a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AI\\002-nlp-lab\\002-imdb-tapt-optuna\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\snu29\\.cache\\huggingface\\hub\\models--albert-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 모델은 아래 모델을 사용하시면 됩니다!\n",
    "# google/bert_uncased_L-4_H-256_A-4\n",
    "# albert-base-v2\n",
    "\n",
    "model_name = \"albert-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 토큰화 함수를 만듭니다.\n",
    "def tokenize_function(sentences):\n",
    "    return tokenizer(\n",
    "        sentences[\"text\"],\n",
    "        truncation=True,       # 문장이 max_length보다 길면 자릅니다.\n",
    "        padding=\"max_length\",  # 문장이 짧으면 0(pad)으로 채워 길이를 맞춥니다.\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5099139",
   "metadata": {},
   "source": [
    "## **5. 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a51db",
   "metadata": {},
   "source": [
    "### **5-1. TAPT**\n",
    "* MLM(Masked Language Model) 기반 TAPT(Tasked Adaptive Pretraining)를 적용하는 단계입니다.\n",
    "* 이 단계에서는 IMDB 데이터셋의 도메인을 미리 학습시켜, 본 학습(Fine-tuning)의 효율을 높입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120af636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked LM 모델을 불러옵니다.\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "# GPU를 사용할 수 있는 환경이면 GPU를 사용해서 모델을 학습시킵니다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 사전 학습된 모델을 가져옵니다.\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# [TODO 1-1] X_train_clean과 X_val_clean을 concat해서 TAPT 학습용 데이터셋을 만듭니다.\n",
    "tapt_df = None\n",
    "\n",
    "# [TODO 1-2] Hugging Face Dataset 포맷으로 변환합니다.\n",
    "tapt_dataset = None\n",
    "\n",
    "# [TODO 1-3] tapt dataset에 대하여 토큰화를 수행합니다.\n",
    "tokenized_tapt_dataset = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d02c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# 데이터셋의 15%를 마스킹해줍니다.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# TAPT 학습을 위한 기본 설정입니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tapt_output\",      # 결과물이 지정될 경로를 지정합니다.\n",
    "    overwrite_output_dir=True,       # 덮어쓰기를 허용합니다.\n",
    "    num_train_epochs=3,              # TAPT는 오래 학습하지 않아도 됩니다.\n",
    "    per_device_train_batch_size=64,  # OOM(Out-Of-Memory)가 발생하면, 배치 사이즈를 줄여주세요!\n",
    "    learning_rate=2e-5,              # # 사전 학습된 모델을 망가뜨리지 않도록 작은 학습률을 사용합니다.\n",
    "    logging_steps=100,               # 중간중간 학습 로그를 보여줍니다.\n",
    "    save_strategy=\"epoch\",           # 매 에폭마다 저장합니다.\n",
    "    save_total_limit=1,              # 가장 최신 모델 1개만 남기고 이전 체크포인트를 삭제합니다.\n",
    "    fp16=True,                       # GPU 메모리를 절약합니다.\n",
    ")\n",
    "\n",
    "# Trainer를 정의합니다.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_tapt_dataset\n",
    ")\n",
    "\n",
    "# TAPT 학습을 시작합니다.\n",
    "trainer.train()\n",
    "\n",
    "# 학습된 모델을 저장합니다.\n",
    "trainer.save_model(\"./tapt_output\")\n",
    "\n",
    "# 토크나이저도 같은 폴더에 저장합니다.\n",
    "tokenizer.save_pretrained(\"./tapt_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56d19d",
   "metadata": {},
   "source": [
    "### **5-2. Optuna로 최적의 하이퍼파리미터 탐색**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO 2-1] X_train_clean, X_val_clean, y_train, y_val을 사용하여 데이터프레임을 만듭니다.\n",
    "train_df = None\n",
    "val_df = None\n",
    "\n",
    "# [TODO 2-2] Hugging Face Dataset 포맷으로 변환합니다.\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "\n",
    "# [TODO 2-3] 각 dataset에 대하여 토큰화를 수행합니다.\n",
    "tokenized_train_dataset = None\n",
    "tokenized_val_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79449597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "TAPT_MODEL_PATH = \"./tapt_output\"\n",
    "\n",
    "# 모델 초기화 합수를 정의합니다.\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        TAPT_MODEL_PATH,\n",
    "        num_labels=2\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4117fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# [TODO 3] 평가 지표를 정의합니다.\n",
    "def compute_metrics(p):\n",
    "    # p는 (모델이 예측한 점수들, 실제 정답)이 묶여 있는 튜플입니다.\n",
    "    \"\"\"\n",
    "    predictions = [\n",
    "      [2.5, -1.2],  # 첫 번째 문장: 0번(부정)일 점수가 높음\n",
    "      [-0.5, 3.1],  # 두 번째 문장: 1번(긍정)일 점수가 높음\n",
    "      [0.1, 0.2]    # 세 번째 문장: 1번(긍정)일 점수가 아주 조금 더 높음\n",
    "    ]\n",
    "\n",
    "    labels = [0, 1, 0]  # 실제 정답: 부정, 긍정, 부정\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "\n",
    "    # [TODO 3-1] 예측한 값 중에 가장 큰 값을 선택합니다.\n",
    "    # 힌트: np.argmax()를 사용하세요.\n",
    "    preds = None\n",
    "\n",
    "    # [TODO 3-2] labels, preds를 사용하여 정확도를 계산합니다.\n",
    "    # 힌트: accuracy_score()를 사용하세요.\n",
    "    accuracy = None\n",
    "\n",
    "    return {f\"accuracy: {accuracy: .4f}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24347ddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (507906089.py, line 22)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31meval_dataset=tokenized_val_dataset\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# 파인튜닝을 위한 기본 설정입니다.\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./finetune_output\",  # 결과물이 지정될 경로를 지정합니다.\n",
    "    overwrite_output_dir=True,       # 덮어쓰기를 허용합니다.\n",
    "    num_train_epochs=3,              # 파인튜닝은 이미 학습되어 있는 모델을 다듬는 과정이라 3~5 에폭이면 충분합니다.\n",
    "    per_device_train_batch_size=64,  # OOM(Out-Of-Memory)가 발생하면, 배치 사이즈를 줄여주세요!\n",
    "    learning_rate=2e-5,              # 사전 학습된 모델을 망가뜨리지 않도록 작은 학습률을 사용합니다.\n",
    "    logging_steps=100,               # 중간중간 학습 로그를 보여줍니다.\n",
    "    save_strategy=\"epoch\",           # 매 에폭마다 저장합니다.\n",
    "    save_total_limit=1,              # 가장 최신 모델 1개만 남기고 이전 체크포인트를 삭제합니다.\n",
    "    fp16=True,                       # GPU 메모리를 절약합니다.\n",
    "    evaluation_strategy=\"epoch\",     # 에폭 단위로 평가를 진행합니다.\n",
    "    load_best_model_at_end=True,     # 끝나면 가장 좋았던 모델을 선택합니다.\n",
    "    metric_for_best_model=\"accuracy\" # 정확도를 평가 지표로 사용합니다.\n",
    ")\n",
    "\n",
    "# Trainer를 정의합니다.\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    model_init=model_init,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6652b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna가 탐색할 범위를 지정합니다.\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),  # learning_rate 탐색 범위를 바꿔보세요!\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 4)               # epochs 탐색 범위도 바꿔가며 실험해보세요!\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",  # accuracy가 최대화되도록 튜닝합니다.\n",
    "    hp_space=hp_space,     # 위에서 정해둔 하이퍼파라미터 탐색 함수입니다.\n",
    "    backend=\"optuna\",\n",
    "    n_trials=10            # 실험 횟수입니다.\n",
    ")\n",
    "\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97531b97",
   "metadata": {},
   "source": [
    "### **5-3. 최적의 하이퍼파라미터로 최종 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299fce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 찾은 최적의 파라미터를 Trainer에 적용합니다.\n",
    "for name, value in best_run.hyperparameters.items():\n",
    "    \"\"\"\n",
    "    name: 하이퍼파라미터 종류\n",
    "    value: 하이퍼파라미터 값\n",
    "    e.g., name = \"learning_rate\n",
    "    e.g., value = \"3e-5\n",
    "    \"\"\"\n",
    "    setattr(trainer.args, name, value)\n",
    "\n",
    "# 최종 학습을 시작합니다.\n",
    "trainer.train()\n",
    "\n",
    "# 검증 지표를 확인합니다.\n",
    "metrics = trainer.evaluate()\n",
    "accuracy = metrics[\"eval_accuracy\"]\n",
    "print(f\"검증 정확도: {accuracy: .4f}\")\n",
    "\n",
    "# 최종 학습된 모델을 저장합니다.\n",
    "trainer.save_model(\"./final_best_model\")\n",
    "\n",
    "# 토크나이저도 같은 폴더에 저장합니다.\n",
    "tokenizer.save_pretrained(\"./final_best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba41b2",
   "metadata": {},
   "source": [
    "## **6. 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b04660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋을 만들어줍니다,\n",
    "test_df = pd.DataFrame({\"text\": X_test_clean, \"label\": y_test})\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(tokenized_test_dataset)\n",
    "accuracy = metrics[\"test_accuracy\"]\n",
    "print(f\"최종 테스트 정확도: {accuracy: .4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
